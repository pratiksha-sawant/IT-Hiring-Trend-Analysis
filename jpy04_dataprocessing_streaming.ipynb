{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing:\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.message = 'This is init'\n",
    "        \n",
    "    def datareading(self):\n",
    "        bucket1 = 's3://hiringtrendanalysis/jobdesc.csv' # already created on S3\n",
    "        client = boto3.resource('s3')\n",
    "        df_jobdesc = pd.read_csv(bucket1)\n",
    "        df_jobdesc = df_jobdesc.drop('Unnamed: 0', axis=1)\n",
    "        df_jobdesc.dropna(inplace = True)\n",
    "        \n",
    "        bucket2 = 's3://hiringtrendanalysis/techwords.csv'\n",
    "        client = boto3.resource('s3')\n",
    "        df_keywords = pd.read_csv(bucket2)\n",
    "#         df_keywords = df_keywords.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        return df_jobdesc,df_keywords\n",
    "    \n",
    "        #tokenize the sentences into words\n",
    "    \n",
    "    def tokenizedWords(self,corpus):\n",
    "#         unigrams = []\n",
    "#         bigrams = []\n",
    "        tokenizedWords = []\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        #Unigrams\n",
    "        for para in corpus:\n",
    "            words = nltk.word_tokenize(para)\n",
    "            for w in words:\n",
    "                if w not in stop_words and len(w)>1 and not w.isdigit():\n",
    "                    tokenizedWords.append(w.lower().strip())\n",
    "                \n",
    "        #Bigrams\n",
    "        for para in corpus:\n",
    "            words = nltk.word_tokenize(para)\n",
    "            fword= []\n",
    "            for w in words:\n",
    "                if w not in stop_words and len(w)>1 and not w.isdigit():\n",
    "                    fword.append(w)\n",
    "#             print(fword)\n",
    "            \n",
    "            nList = list(ngrams(fword,2))\n",
    "            \n",
    "            for w in nList:\n",
    "#                 print(w)\n",
    "                tmp = ''.join(w)\n",
    "                tokenizedWords.append(tmp.lower().strip())\n",
    "\n",
    "        return tokenizedWords\n",
    "    \n",
    "    def datacleansing(self,df_jobdesc,df_keywords ):\n",
    "        \n",
    "        jd_tokens = []\n",
    "        techwords = []\n",
    "        \n",
    "        for index, row in df_keywords.iterrows():\n",
    "            techwords.append(row['words'])\n",
    "            \n",
    "#         print(techwords)\n",
    "        for index, row in df_jobdesc.iterrows():\n",
    "            corpus = nltk.sent_tokenize(row['job_desc'])\n",
    "        \n",
    "            for i in range(len(corpus)):\n",
    "                corpus [i] = corpus [i].lower()\n",
    "                corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "                corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
    "            \n",
    "            jd_tk = self.tokenizedWords(corpus)\n",
    "            \n",
    "            for i in jd_tk:\n",
    "                if i in techwords:\n",
    "#                     print(i)\n",
    "                    jd_tokens.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            row['job_desc'] = jd_tokens\n",
    "            \n",
    "            jd_tokens = []\n",
    "        \n",
    "        return df_jobdesc \n",
    "\n",
    "    \n",
    "    def wordDictionary(self,df_keywords):\n",
    "        words_dict = {}\n",
    "        #create a dictionary for words in corpus with count\n",
    "        #words_dict = dict(zip(df_keywords.words, count))\n",
    "        for index, row in df_keywords.iterrows():\n",
    "            words_dict.update({row['words']:0})\n",
    "            \n",
    "        return words_dict\n",
    "        \n",
    "    def countWords(self,mydata,df_keywords):\n",
    "    \n",
    "        count = 0\n",
    "        techwords = []\n",
    "        final = {}\n",
    "        \n",
    "        #read final tech words\n",
    "        for index, row in df_keywords.iterrows():\n",
    "            techwords.append(row['words'])\n",
    "        \n",
    "        \n",
    "        #code change required\n",
    "        for index, row in mydata.iterrows():\n",
    "            words_dict = self.wordDictionary(df_keywords)\n",
    "            \n",
    "            for r in row['job_desc']:\n",
    "                if r in techwords:\n",
    "#                     print(r,str(words_dict.get(r)))\n",
    "                    count = words_dict.get(r) + 1\n",
    "#                     print(count)\n",
    "                    words_dict.update({r : count})\n",
    "            \n",
    "            key = index\n",
    "            final.update({key:words_dict})\n",
    "            \n",
    "#             for i, j in final.items():\n",
    "#                 print(j)\n",
    "        \n",
    "        df_wordcount = pd.DataFrame.from_dict(final, orient = 'index')\n",
    "        \n",
    "        df_merged = mydata.merge(df_wordcount, left_index=True, right_index=True)\n",
    "\n",
    "        return df_merged\n",
    "#             print(final)\n",
    "    \n",
    "    def uploadToS3(self,result):\n",
    "        try:\n",
    "            bucket = 'hiringtrendanalysis' # already created on S3\n",
    "            csv_buffer = StringIO()\n",
    "            result.to_csv(csv_buffer)\n",
    "            s3_resource = boto3.resource('s3')\n",
    "            s3_resource.Object(bucket, 'wordcount.csv').put(Body=csv_buffer.getvalue())\n",
    "        except ClientError as e:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "        return True\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Streaming:\n",
    "    def __init__(self):\n",
    "        self.message = 'This is init'\n",
    "        \n",
    "    def createClient(self, service, region):\n",
    "        return boto3.client(service, region_name = region)\n",
    "    \n",
    "    def loadData(self):\n",
    "        bucket1 = 's3://hiringtrendanalysis/wordcount.csv' # already created on S3\n",
    "        client = boto3.resource('s3')\n",
    "        df_data = pd.read_csv(bucket1)\n",
    "        return df_data\n",
    "    \n",
    "    def sendKinesis(self,kinesis_client, kinesis_stream_name, kinesis_shard_count, data):\n",
    "        kinesis_records = []\n",
    "        \n",
    "        (rows, columns) = data.shape\n",
    "        \n",
    "        currentBytes = 0\n",
    "        \n",
    "        rowCount = 0\n",
    "        \n",
    "        totalRowCount = rows\n",
    "        \n",
    "        sendKinesis = False\n",
    "        \n",
    "        shardCount = 1\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            values = '|'.join(str(value) for value in row)\n",
    "            \n",
    "            encodedValues = bytes(values, 'utf-8')\n",
    "            \n",
    "            kinesis_record = {\"Data\": encodedValues, \n",
    "                             \"PartitionKey\" : str(shardCount)}\n",
    "            \n",
    "            kinesis_records.append(kinesis_record)\n",
    "            \n",
    "            stringBytes = len(values.encode('utf-8'))\n",
    "            currentBytes = currentBytes + stringBytes \n",
    "            \n",
    "            if len(kinesis_records) == 500:\n",
    "                sendKinesis = True\n",
    "                \n",
    "            if currentBytes > 50000:\n",
    "                sendKinesis = True\n",
    "            \n",
    "            \n",
    "            if rowCount == totalRowCount:\n",
    "                sendKinesis = True\n",
    "                \n",
    "            if sendKinesis == True:\n",
    "                \n",
    "                response = kinesis_client.put_records(Records = kinesis_records,   StreamName = kinesis_stream_name )\n",
    "            \n",
    "                kinesis_records = []\n",
    "                sendKinesis = False\n",
    "                currentBytes = 0\n",
    "                \n",
    "                shardCount = shardCount + 1\n",
    "                \n",
    "                if shardCount > kinesis_shard_count:\n",
    "                    shardCount = 1\n",
    "                    \n",
    "            rowCount += 1\n",
    "            \n",
    "            # log out how many records were pushed\n",
    "        print('Total Records sent to Kinesis: {0}'.format(totalRowCount))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records sent to Kinesis: 6227\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    proc = Processing()\n",
    "    df_jobdesc,df_keywords = proc.datareading()\n",
    "    \n",
    "    mydata = proc.datacleansing(df_jobdesc,df_keywords)\n",
    "    data = proc.countWords(mydata,df_keywords)\n",
    "    \n",
    "    proc.uploadToS3(data)\n",
    "    \n",
    "    stream = Streaming()\n",
    "    \n",
    "    kinesis_client = stream.createClient('kinesis', 'us-east-1')\n",
    "    \n",
    "    data = stream.loadData()\n",
    "    \n",
    "    # send it to kinesis data stream\n",
    "    stream_name = \"hiring_trend_analysis\"\n",
    "    stream_shard_count = 1\n",
    "    \n",
    "    stream.sendKinesis(kinesis_client, stream_name, stream_shard_count, data) # send it!\n",
    "    \n",
    "#     d.to_csv('test.csv')\n",
    "    \n",
    "#     print(a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
