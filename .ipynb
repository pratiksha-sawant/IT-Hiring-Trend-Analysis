{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing:\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.message = 'This is init'\n",
    "        \n",
    "    def datareading(self):\n",
    "        bucket1 = 's3://hiringtrendanalysis/jobdesc.csv' # already created on S3\n",
    "        client = boto3.resource('s3')\n",
    "        df_jobdesc = pd.read_csv(bucket1)\n",
    "        df_jobdesc = df_jobdesc.drop('Unnamed: 0', axis=1)\n",
    "        df_jobdesc.dropna(inplace = True)\n",
    "        \n",
    "        bucket2 = 's3://hiringtrendanalysis/techwords.csv'\n",
    "        client = boto3.resource('s3')\n",
    "        df_keywords = pd.read_csv(bucket2)\n",
    "#         df_keywords = df_keywords.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        return df_jobdesc,df_keywords\n",
    "    \n",
    "        #tokenize the sentences into words\n",
    "    \n",
    "    def tokenizedWords(self,corpus):\n",
    "#         unigrams = []\n",
    "#         bigrams = []\n",
    "        tokenizedWords = []\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "        #Unigrams\n",
    "        for para in corpus:\n",
    "            words = nltk.word_tokenize(para)\n",
    "            for w in words:\n",
    "                if w not in stop_words and len(w)>1 and not w.isdigit():\n",
    "                    tokenizedWords.append(w.lower().strip())\n",
    "                \n",
    "        #Bigrams\n",
    "        for para in corpus:\n",
    "            words = nltk.word_tokenize(para)\n",
    "            fword= []\n",
    "            for w in words:\n",
    "                if w not in stop_words and len(w)>1 and not w.isdigit():\n",
    "                    fword.append(w)\n",
    "#             print(fword)\n",
    "            \n",
    "            nList = list(ngrams(fword,2))\n",
    "            \n",
    "            for w in nList:\n",
    "#                 print(w)\n",
    "                tmp = ''.join(w)\n",
    "                tokenizedWords.append(tmp.lower().strip())\n",
    "\n",
    "        return tokenizedWords\n",
    "    \n",
    "    def datacleansing(self,df_jobdesc,df_keywords ):\n",
    "        \n",
    "        jd_tokens = []\n",
    "        techwords = []\n",
    "        \n",
    "        for index, row in df_keywords.iterrows():\n",
    "            techwords.append(row['words'])\n",
    "            \n",
    "#         print(techwords)\n",
    "        for index, row in df_jobdesc.iterrows():\n",
    "            corpus = nltk.sent_tokenize(row['job_desc'])\n",
    "        \n",
    "            for i in range(len(corpus)):\n",
    "                corpus [i] = corpus [i].lower()\n",
    "                corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "                corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n",
    "            \n",
    "            jd_tk = self.tokenizedWords(corpus)\n",
    "            \n",
    "            for i in jd_tk:\n",
    "                if i in techwords:\n",
    "#                     print(i)\n",
    "                    jd_tokens.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            row['job_desc'] = jd_tokens\n",
    "            \n",
    "            jd_tokens = []\n",
    "        \n",
    "        return df_jobdesc \n",
    "\n",
    "    \n",
    "    def wordDictionary(self,df_keywords):\n",
    "        words_dict = {}\n",
    "        #create a dictionary for words in corpus with count\n",
    "        #words_dict = dict(zip(df_keywords.words, count))\n",
    "        for index, row in df_keywords.iterrows():\n",
    "            words_dict.update({row['words']:0})\n",
    "            \n",
    "        return words_dict\n",
    "        \n",
    "    def countWords(self,mydata,df_keywords):\n",
    "    \n",
    "        count = 0\n",
    "        techwords = []\n",
    "        final = {}\n",
    "        \n",
    "        #read final tech words\n",
    "        for index, row in df_keywords.iterrows():\n",
    "            techwords.append(row['words'])\n",
    "        \n",
    "        \n",
    "        #code change required\n",
    "        for index, row in mydata.iterrows():\n",
    "            words_dict = self.wordDictionary(df_keywords)\n",
    "            \n",
    "            for r in row['job_desc']:\n",
    "                if r in techwords:\n",
    "#                     print(r,str(words_dict.get(r)))\n",
    "                    count = words_dict.get(r) + 1\n",
    "#                     print(count)\n",
    "                    words_dict.update({r : count})\n",
    "            \n",
    "            key = index\n",
    "            final.update({key:words_dict})\n",
    "            \n",
    "            for i, j in final.items():\n",
    "                print(j)\n",
    "        \n",
    "        df_wordcount = pd.DataFrame.from_dict(final, orient = 'index')\n",
    "        \n",
    "        df_merged = mydata.merge(df_wordcount, left_index=True, right_index=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        return df_merged.reset_index(drop=True, inplace=True)\n",
    "#             print(final)\n",
    "    \n",
    "    def uploadToS3(self,result):\n",
    "        try:\n",
    "            bucket = 'hiringtrendanalysis' # already created on S3\n",
    "            csv_buffer = StringIO()\n",
    "            result.to_csv(csv_buffer)\n",
    "            s3_resource = boto3.resource('s3')\n",
    "            s3_resource.Object(bucket, 'WordCount.csv').put(Body=csv_buffer.getvalue())\n",
    "        except ClientError as e:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "        \n",
    "# class Streaming:\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    proc = Processing()\n",
    "    df_jobdesc,df_keywords = proc.datareading()\n",
    "    \n",
    "    mydata = proc.datacleansing(df_jobdesc,df_keywords)\n",
    "    data = proc.countWords(mydata,df_keywords)\n",
    "    \n",
    "    proc.uploadToS3(data)\n",
    "    \n",
    "#     d.to_csv('test.csv')\n",
    "    \n",
    "#     print(a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
