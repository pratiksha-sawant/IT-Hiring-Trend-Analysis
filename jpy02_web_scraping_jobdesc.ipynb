{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#data processing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#AWS\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.message = 'This is init'\n",
    "\n",
    "    def extract_job_info(self,soup, job_info,role):\n",
    "    #     job_info = {}\n",
    "        temp = []\n",
    "        jobdesc = ''\n",
    "        mainpageURL = '' \n",
    "        count = len(job_info)\n",
    "        for div in soup.find_all(name=\"div\", attrs={\"class\":\"jobsearch-SerpJobCard\"}):\n",
    "\n",
    "            for subdiv in div.find_all(name=\"div\", attrs={\"class\":\"sjcl\"}):\n",
    "                try:\n",
    "                    s = subdiv.find(name=\"span\", attrs={\"class\":\"company\"})\n",
    "                    temp.append((s.text).strip()) #fetches company names (3)\n",
    "                except:\n",
    "                    temp.append(\"NA\")\n",
    "\n",
    "                try:\n",
    "                    d = subdiv.find(name=\"div\", attrs={\"class\":\"recJobLoc\"})\n",
    "                    temp.append((d['data-rc-loc']).strip()) #fetches location (4)\n",
    "                except:\n",
    "                    temp.append(\"NA\")\n",
    "    #         print(div)\n",
    "\n",
    "            for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "\n",
    "                try:\n",
    "                    temp.append((a['title']).strip()) #fetches title (1)\n",
    "                except:\n",
    "                    temp.append(\"NA\")\n",
    "\n",
    "                try:\n",
    "                    mainpageURL = str(\"https://www.indeed.com\" + a[\"href\"])\n",
    "    #                 print(mainpageURL)\n",
    "                    desc_page = requests.get(mainpageURL)\n",
    "                    page_soup = BeautifulSoup(desc_page.text, \"html.parser\")\n",
    "                    for dv in page_soup.find_all(name=\"div\", attrs={\"id\":\"jobDescriptionText\"}):\n",
    "                        for l in dv.find_all(\"li\"):\n",
    "                            jobdesc = jobdesc +''+ l.text\n",
    "    #                     print(jobdesc)\n",
    "                        if jobdesc=='':\n",
    "                            temp.append('NA')\n",
    "                        else:\n",
    "                            temp.append(jobdesc)      #fetches job description (2)\n",
    "                except:\n",
    "                    temp.append('NA')\n",
    "\n",
    "\n",
    "            temp.append(role)\n",
    "\n",
    "            job_info[count] = temp\n",
    "            count = count + 1\n",
    "            temp = []\n",
    "            jobdesc = ''\n",
    "\n",
    "        return(job_info)\n",
    "\n",
    "    # job_info = extract_job_info(soup)\n",
    "\n",
    "    def htmlParser(self,role):\n",
    "        if role == 'software engineer':\n",
    "            URL = \"https://www.indeed.com/jobs?q=software+engineer&l=United+States\"\n",
    "            tempURL = URL\n",
    "            num = 10\n",
    "            job_info = {}\n",
    "            for i in range(0,100):\n",
    "                page = requests.get(URL)\n",
    "    #             print(page)\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    #             print(soup.prettify())\n",
    "                URL = tempURL +'&start='+str(num)\n",
    "    #             print(URL)\n",
    "                num = num + 10\n",
    "                job_info = self.extract_job_info(soup,job_info,role)\n",
    "\n",
    "        elif role == 'data scientist':\n",
    "            URL = \"https://www.indeed.com/jobs?q=data+scientist&l=United+States\"\n",
    "            tempURL = URL\n",
    "            num = 10\n",
    "            job_info = {}\n",
    "            for i in range(0,100):\n",
    "                page = requests.get(URL)\n",
    "    #             print(page)\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    #             print(soup.prettify())\n",
    "                URL = tempURL +'&start='+str(num)\n",
    "    #             print(URL)\n",
    "                num = num + 10\n",
    "                job_info = self.extract_job_info(soup,job_info,role)\n",
    "\n",
    "        elif role == 'data analyst':\n",
    "            URL = \"https://www.indeed.com/jobs?q=data+analyst&l=United+States\"\n",
    "            tempURL = URL\n",
    "            num = 10\n",
    "            job_info = {}\n",
    "            for i in range(0,100):\n",
    "                page = requests.get(URL)\n",
    "    #             print(page)\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    #             print(soup.prettify())\n",
    "                URL = tempURL +'&start='+str(num)\n",
    "    #             print(URL)\n",
    "                num = num + 10\n",
    "                job_info = self.extract_job_info(soup,job_info,role)\n",
    "\n",
    "        elif role == 'machine learning engineer':\n",
    "            URL = \"https://www.indeed.com/jobs?q=machine+learning+engineer&l=United+States\"\n",
    "            tempURL = URL\n",
    "            num = 10\n",
    "            job_info = {}\n",
    "            for i in range(0,100):\n",
    "                page = requests.get(URL)\n",
    "    #             print(page)\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    #             print(soup.prettify())\n",
    "                URL = tempURL +'&start='+str(num)\n",
    "    #             print(URL)\n",
    "                num = num + 10\n",
    "                job_info = self.extract_job_info(soup,job_info,role)\n",
    "\n",
    "        else:\n",
    "            print('Enter a valid role')\n",
    "\n",
    "        return job_info\n",
    "\n",
    "\n",
    "\n",
    "class DumpData:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.message = 'This is init'\n",
    "            \n",
    "    def uploadToS3(self,result):\n",
    "        try:\n",
    "            bucket = 'hiringtrendanalysis' # already created on S3\n",
    "            csv_buffer = StringIO()\n",
    "            result.to_csv(csv_buffer)\n",
    "            s3_resource = boto3.resource('s3')\n",
    "            s3_resource.Object(bucket, 'jobdesc.csv').put(Body=csv_buffer.getvalue())\n",
    "        except ClientError as e:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def mergeDataframes(self,swe_info,da_info,ds_info,ml_info):\n",
    "        swe_info_df = pd.DataFrame.from_dict(swe_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "        da_info_df = pd.DataFrame.from_dict(da_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "        ds_info_df = pd.DataFrame.from_dict(ds_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "        ml_info_df = pd.DataFrame.from_dict(ml_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "        \n",
    "        frames = [swe_info_df,da_info_df,ds_info_df,ml_info_df]\n",
    "        result = pd.concat(frames)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "1854\n",
      "1692\n",
      "1855\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    webscraper = WebScraper()\n",
    "    swe_info = webscraper.htmlParser('software engineer')\n",
    "    da_info =  webscraper.htmlParser('data analyst')\n",
    "    ds_info =  webscraper.htmlParser('data scientist')\n",
    "    ml_info =  webscraper.htmlParser('machine learning engineer')\n",
    "    print(len(swe_info))\n",
    "    print(len(ds_info))\n",
    "    print(len(da_info))\n",
    "    print(len(ml_info))\n",
    "    \n",
    "    dmpdata = DumpData()\n",
    "    result = dmpdata.mergeDataframes(swe_info,da_info,ds_info,ml_info)\n",
    "    dmpdata.uploadToS3(result)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swe_info = htmlParser('software engineer')\n",
    "# print(len(swe_info))\n",
    "# swe_info[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping data analyst jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_info = htmlParser('data analyst')\n",
    "# print(len(da_info))\n",
    "# da_info[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping data scientist jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_info =  htmlParser('data scientist')\n",
    "# print(len(ds_info))\n",
    "# ds_info[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping machine learning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_info =  htmlParser('machine learning engineer')\n",
    "# print(len(ml_info))\n",
    "# ml_info[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swe_info_df = pd.DataFrame.from_dict(swe_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "# swe_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_info_df = pd.DataFrame.from_dict(da_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "# da_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_info_df = pd.DataFrame.from_dict(ds_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "# ds_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_info_df = pd.DataFrame.from_dict(ml_info, orient='index', columns=['company','location','title','job_desc','role'])\n",
    "# ml_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = [swe_info_df,da_info_df,ds_info_df,ml_info_df]\n",
    "# result = pd.concat(frames)\n",
    "# # result.head()\n",
    "# # result.to_csv('temp.csv', index=False)\n",
    "# uploadToS3(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://www.indeed.com/jobs?q=software+engineer&l=United+States\"\n",
    "\n",
    "# page = requests.get(URL)\n",
    "# #             print(page)\n",
    "# soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "# #             print(soup.prettify())\n",
    "\n",
    "# jb = {}\n",
    "# job_info = {}\n",
    "\n",
    "# jb = extract_job_info(soup,job_info)\n",
    "\n",
    "# jb[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
